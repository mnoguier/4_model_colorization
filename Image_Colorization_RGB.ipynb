{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "evegaCopy of Image_Colorization_RGB.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "a4bZpufUuFgr",
        "eB9uoUr7mWQm",
        "zhKzYqZEn44U"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Image Colorization in the RGB Colorspace\n",
        "\n"
      ],
      "metadata": {
        "id": "BJQIXEB6h_B_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqtg5XgxvXN9",
        "outputId": "7e6ef160-2057-42a1-917b-f29ddda4ab13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/My Drive/Object Detection/Image Colorization/landscape_images.zip\" -d \"/content/drive/My Drive/Object Detection/Image Colorization/landscape_images/\""
      ],
      "metadata": {
        "id": "RIaE2A91dVpv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "a28e3a20-75b8-40e3-9b00-b92d6cf811e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/My Drive/Object Detection/Image Colorization/landscape_images.zip\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-8a4f064cc3d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unzip \"/content/drive/My Drive/Object Detection/Image Colorization/landscape_images.zip\" -d \"/content/drive/My Drive/Object Detection/Image Colorization/landscape_images/\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    445\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m   result = _run_command(\n\u001b[0;32m--> 447\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    448\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    197\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_monitor_process\u001b[0;34m(parent_pty, epoll, p, cmd, update_stdin_widget)\u001b[0m\n\u001b[1;32m    227\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_poll_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_poll_process\u001b[0;34m(parent_pty, epoll, p, cmd, decoder, state)\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;31m# TODO(b/115527726): Rather than sleep, poll for incoming messages from\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;31m# the frontend in the same poll as for the output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "4gTsoNfnvX7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/MyDrive/Object Detection/Image Colorization'\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpMR0ddmvYHE",
        "outputId": "7a32ff47-09bf-416e-ab16-552e09bd3aac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1l9lgKWCgTf4rXpuIuwGAHyEDtRWf-MtR/Object Detection/Image Colorization\n",
            " BasicModel-epoch-44.pt\n",
            " basic_model.py\n",
            " \u001b[0m\u001b[01;34mbasicNet_files\u001b[0m/\n",
            " basicNet.html\n",
            " ColorizationModel-epoch-50.pt\n",
            " colorize_data.py\n",
            "'evegaCopy of Image_Colorization_RGB.ipynb'\n",
            " \u001b[01;34mimages\u001b[0m/\n",
            " \u001b[01;34mlandscape_images\u001b[0m/\n",
            " landscape_images.zip\n",
            " \u001b[01;34moutputs\u001b[0m/\n",
            " \u001b[01;34m__pycache__\u001b[0m/\n",
            " rename.py\n",
            " ResNetUNetModel-epoch-after-26-24.pt\n",
            "'swatibCopy of Image_Colorization_RGB.ipynb'\n",
            " unet_diagram.webp\n",
            " UNetModel-epoch-34.pt\n",
            " UnetResnet_diagram.png\n",
            "'(Use_me_1)_Image_Colorization_RGB.ipynb'\n",
            "'(Use_this_2)Image_Colorization_RGB.ipynb'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip '//content//drive//My Drive//442 Project//Image Colorisation//landscape_images.zip' -d '//content//drive//My Drive//442 Project//Image Colorisation//images'\n"
      ],
      "metadata": {
        "id": "naitqgfFwaJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the necessary modules "
      ],
      "metadata": {
        "id": "577aBVJ_i9O5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUteedQ-arw5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from colorize_data import *\n",
        "from torch.nn import *\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torchvision import datasets, transforms\n",
        "import os, shutil, time\n",
        "from collections import OrderedDict\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing the data for training \n",
        " \n",
        "1. Load data from the image folder and create train and val dataloaders"
      ],
      "metadata": {
        "id": "etbW8n1UjBdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Dataloader\n",
        "training_data = ColorizeData('/content/drive/MyDrive/Object Detection/Image Colorization/landscape_images/landscape_images/train')\n",
        "train_dataloader = DataLoader(training_data, batch_size=32, shuffle=True)\n",
        "\n",
        "\n",
        "# Validation Dataloader\n",
        "validation_data = ColorizeData('/content/drive/MyDrive/Object Detection/Image Colorization/landscape_images/landscape_images/val')\n",
        "val_dataloader = DataLoader(validation_data, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "mbGl2jS0kWK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the model "
      ],
      "metadata": {
        "id": "hARkTxL6kl5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Basic Model "
      ],
      "metadata": {
        "id": "a4bZpufUuFgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicNet(nn.Module):\n",
        "  \n",
        "    def __init__(self, d=128):\n",
        "        super(BasicNet, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1) \n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1) \n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1) \n",
        "        self.batchnorm1 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1) \n",
        "        self.conv5 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1) \n",
        "\n",
        "        self.convTrans1 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1) \n",
        "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.convTrans2 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1) \n",
        "        self.batchnorm3 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.convTrans3 = nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1) \n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        x = F.relu(self.conv1(input))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.batchnorm1(self.conv4(x)))\n",
        "        x = F.relu(self.conv5(x))\n",
        "        x = F.relu(self.batchnorm2(self.convTrans1(x)))\n",
        "        x = F.relu(self.batchnorm3(self.convTrans2(x)))\n",
        "        x = self.convTrans3(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "7sPdS9JduWjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Colorization Model with Resnet Encoder"
      ],
      "metadata": {
        "id": "eB9uoUr7mWQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "class ColorizationModel(nn.Module):\n",
        "  def __init__(self, input_size=256):\n",
        "    super(ColorizationModel, self).__init__()\n",
        "    \n",
        "    # ResNet - First layer accepts grayscale images, \n",
        "    resnet = models.resnet18(num_classes=100)\n",
        "    resnet.conv1.weight = nn.Parameter(resnet.conv1.weight.sum(dim=1).unsqueeze(1)) \n",
        "    self.midlevel_resnet = nn.Sequential(*list(resnet.children())[0:6])\n",
        "    RESNET_FEATURE_SIZE = 128\n",
        "\n",
        "    ## Upsampling Network\n",
        "    self.upsample = nn.Sequential(     \n",
        "      nn.Conv2d(RESNET_FEATURE_SIZE, 128, kernel_size=3, stride=1, padding=1),\n",
        "      nn.BatchNorm2d(128),\n",
        "      nn.ReLU(),\n",
        "      nn.Upsample(scale_factor=2),\n",
        "      nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
        "      nn.BatchNorm2d(64),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "      nn.BatchNorm2d(64),\n",
        "      nn.ReLU(),\n",
        "      nn.Upsample(scale_factor=2),\n",
        "      nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),\n",
        "      nn.BatchNorm2d(32),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(32, 3, kernel_size=3, stride=1, padding=1),\n",
        "      nn.Upsample(scale_factor=2)\n",
        "    )\n",
        "\n",
        "  def forward(self, input):\n",
        "    midlevel_features = self.midlevel_resnet(input)\n",
        "    output = self.upsample(midlevel_features)\n",
        "    return output"
      ],
      "metadata": {
        "id": "3kp2U_w3koXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. ResNetUNet Model \n",
        "Uses the first few layers of ResNet in the encoder part and UNet type upsampling in the decoder "
      ],
      "metadata": {
        "id": "zhKzYqZEn44U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models\n",
        "\n",
        "# Define the ConvRelu block which does the Sequential operations of Convolution and ReLU\n",
        "def ConvRelu(in_channels, out_channels, kernel, padding):\n",
        "  return nn.Sequential(\n",
        "    nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
        "    nn.ReLU(inplace=True),\n",
        "  )\n",
        "\n",
        "\n",
        "class ResNetUNet(nn.Module):\n",
        "  def __init__(self, n_class):\n",
        "    super().__init__()\n",
        "\n",
        "    self.base_model = torchvision.models.resnet18(pretrained=True)\n",
        "    self.base_layers = list(self.base_model.children())\n",
        "\n",
        "    self.conv0 = nn.Sequential(*self.base_layers[:3]) \n",
        "    self.convRelu0 = ConvRelu(64, 64, 1, 0)\n",
        "    self.conv1 = nn.Sequential(*self.base_layers[3:5]) \n",
        "    self.convRelu1 = ConvRelu(64, 64, 1, 0)\n",
        "    self.conv2 = self.base_layers[5]  \n",
        "    self.convRelu2 = ConvRelu(128, 128, 1, 0)\n",
        "    self.conv3 = self.base_layers[6]  \n",
        "    self.convRelu3 = ConvRelu(256, 256, 1, 0)\n",
        "    self.conv4 = self.base_layers[7]  \n",
        "    self.convRelu4 = ConvRelu(512, 512, 1, 0)\n",
        "\n",
        "    self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "    self.convRelu_up3 = ConvRelu(256 + 512, 512, 3, 1)\n",
        "    self.convRelu_up2 = ConvRelu(128 + 512, 256, 3, 1)\n",
        "    self.convRelu_up1 = ConvRelu(64 + 256, 256, 3, 1)\n",
        "    self.convRelu_up0 = ConvRelu(64 + 256, 128, 3, 1)\n",
        "\n",
        "    self.convOriginal0 = ConvRelu(3, 64, 3, 1)\n",
        "    self.convOriginal1 = ConvRelu(64, 64, 3, 1)\n",
        "    self.convOriginal2 = ConvRelu(64 + 128, 64, 3, 1)\n",
        "\n",
        "    self.conv_last = nn.Conv2d(64, n_class, 1)\n",
        "\n",
        "  def forward(self, input):\n",
        "    x_original = self.convOriginal0(input)\n",
        "    x_original = self.convOriginal1(x_original)\n",
        "\n",
        "    conv0 = self.conv0(input)\n",
        "    conv1 = self.conv1(conv0)\n",
        "    conv2 = self.conv2(conv1)\n",
        "    conv3 = self.conv3(conv2)\n",
        "    conv4 = self.conv4(conv3)\n",
        "\n",
        "    conv4 = self.convRelu4(conv4)\n",
        "    x = self.upsample(conv4)\n",
        "    conv3 = self.convRelu3(conv3)\n",
        "    x = torch.cat([x, conv3], dim=1)\n",
        "    x = self.convRelu_up3(x)\n",
        "\n",
        "    x = self.upsample(x)\n",
        "    conv2 = self.convRelu2(conv2)\n",
        "    x = torch.cat([x, conv2], dim=1)\n",
        "    x = self.convRelu_up2(x)\n",
        "\n",
        "    x = self.upsample(x)\n",
        "    conv1 = self.convRelu1(conv1)\n",
        "    x = torch.cat([x, conv1], dim=1)\n",
        "    x = self.convRelu_up1(x)\n",
        "\n",
        "    x = self.upsample(x)\n",
        "    conv0 = self.convRelu0(conv0)\n",
        "    x = torch.cat([x, conv0], dim=1)\n",
        "    x = self.convRelu_up0(x)\n",
        "\n",
        "    x = self.upsample(x)\n",
        "    x = torch.cat([x, x_original], dim=1)\n",
        "    x = self.convOriginal2(x)\n",
        "\n",
        "    out = self.conv_last(x)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "lRw25TSVn5zG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. UNet Colorization Model\n",
        "\n",
        "(Adapted from the Pytorch tutorial)"
      ],
      "metadata": {
        "id": "JXnex9Ztm79j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels=1, out_channels=3, init_features=32):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        features = init_features\n",
        "        self.encoder1 = UNet._block(in_channels, features, name=\"enc1\")\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder2 = UNet._block(features, features * 2, name=\"enc2\")\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.bottleneck = UNet._block(features * 8, features * 16, name=\"bottleneck\")\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(features * 16, features * 8, kernel_size=2, stride=2)\n",
        "        self.decoder4 = UNet._block((features * 8) * 2, features * 8, name=\"dec4\")\n",
        "        self.upconv3 = nn.ConvTranspose2d(features * 8, features * 4, kernel_size=2, stride=2)\n",
        "        self.decoder3 = UNet._block((features * 4) * 2, features * 4, name=\"dec3\")\n",
        "        self.upconv2 = nn.ConvTranspose2d(features * 4, features * 2, kernel_size=2, stride=2)\n",
        "        self.decoder2 = UNet._block((features * 2) * 2, features * 2, name=\"dec2\")\n",
        "        self.upconv1 = nn.ConvTranspose2d(features * 2, features, kernel_size=2, stride=2)\n",
        "        self.decoder1 = UNet._block(features * 2, features, name=\"dec1\")\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels=features, out_channels=out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "      \n",
        "        enc1 = self.encoder1(x)\n",
        "        enc2 = self.encoder2(self.pool1(enc1))\n",
        "        enc3 = self.encoder3(self.pool2(enc2))\n",
        "        enc4 = self.encoder4(self.pool3(enc3))\n",
        "\n",
        "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
        "\n",
        "        dec4 = self.upconv4(bottleneck)\n",
        "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
        "        dec4 = self.decoder4(dec4)\n",
        "        dec3 = self.upconv3(dec4)\n",
        "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
        "        dec3 = self.decoder3(dec3)\n",
        "        dec2 = self.upconv2(dec3)\n",
        "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
        "        dec2 = self.decoder2(dec2)\n",
        "        dec1 = self.upconv1(dec2)\n",
        "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
        "        dec1 = self.decoder1(dec1)\n",
        "        return torch.sigmoid(self.conv(dec1))\n",
        "\n",
        "    @staticmethod\n",
        "    def _block(in_channels, features, name):\n",
        "        return nn.Sequential(\n",
        "            OrderedDict(\n",
        "                [\n",
        "                    (\n",
        "                        name + \"conv1\",\n",
        "                        nn.Conv2d(\n",
        "                            in_channels=in_channels,\n",
        "                            out_channels=features,\n",
        "                            kernel_size=3,\n",
        "                            padding=1,\n",
        "                            bias=False,\n",
        "                        ),\n",
        "                    ),\n",
        "                    (name + \"norm1\", nn.BatchNorm2d(num_features=features)),\n",
        "                    (name + \"relu1\", nn.ReLU(inplace=True)),\n",
        "                    (\n",
        "                        name + \"conv2\",\n",
        "                        nn.Conv2d(\n",
        "                            in_channels=features,\n",
        "                            out_channels=features,\n",
        "                            kernel_size=3,\n",
        "                            padding=1,\n",
        "                            bias=False,\n",
        "                        ),\n",
        "                    ),\n",
        "                    (name + \"norm2\", nn.BatchNorm2d(num_features=features)),\n",
        "                    (name + \"relu2\", nn.ReLU(inplace=True)),\n",
        "                ]\n",
        "            )\n",
        "        )"
      ],
      "metadata": {
        "id": "CGBbGpu1nCGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set the hyperparameters"
      ],
      "metadata": {
        "id": "bz6vt5z_kb8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "use_gpu = torch.cuda.is_available()\n",
        "\n",
        "# Initialise the model \n",
        "model = BasicNet()\n",
        "\n",
        "# Define the hyperparameters\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=0.0)\n",
        "criterion = nn.MSELoss()  # Change this to try different loss functions \n",
        "\n",
        "num_epochs = 50\n",
        "best_loss = 1e10\n"
      ],
      "metadata": {
        "id": "sgK17KYebHdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "summary(model.cpu(), input_size=(1, 256, 256))"
      ],
      "metadata": {
        "id": "vqfvFo_p3Vkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting it all together "
      ],
      "metadata": {
        "id": "vELM65UwksO7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Define the metric used to track model performance during training \n",
        "(Adapted from the Pytorch tutorial)\n",
        "\n",
        "2. Define the training function\n",
        "\n",
        "3. Define the validation function"
      ],
      "metadata": {
        "id": "k1hz1B2Ujm8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A handy class from the PyTorch ImageNet tutorial\n",
        "class AverageMeter(object):\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.reset()\n",
        "  \n",
        "  def reset(self):\n",
        "    self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n",
        "  \n",
        "  def update(self, val, n=1):\n",
        "      \n",
        "    self.val = val\n",
        "    self.sum += val * n\n",
        "    self.count += n\n",
        "    self.avg = self.sum / self.count\n",
        "    \n",
        "\n",
        "# Define the validation function \n",
        "def validation(val_loader, model, criterion):\n",
        "    \n",
        "    # Change the model to the eval mode during validation\n",
        "    model.eval()\n",
        "\n",
        "    # Prepare value counters and timers\n",
        "    batch_time, data_time, losses = AverageMeter(), AverageMeter(), AverageMeter()\n",
        "  \n",
        "    end = time.time()\n",
        "    \n",
        "    for i, (input_image, target_image) in enumerate(val_loader):\n",
        "        \n",
        "      data_time.update(time.time() - end)\n",
        "  \n",
        "      # If GPU available then use it\n",
        "      if use_gpu: \n",
        "          \n",
        "          input_image, target_image = input_image.cuda(), target_image.cuda()\n",
        "          model = model.cuda()\n",
        "          criterion = criterion.cuda()\n",
        "          \n",
        "      # Run validation pass on the model  \n",
        "      predicted = model(input_image)\n",
        "      \n",
        "      # Calculate the losses \n",
        "      loss = criterion(predicted, target_image)\n",
        "      losses.update(loss.item(), input_image.size(0))\n",
        "      \n",
        "      # Record time to do forward pass\n",
        "      batch_time.update(time.time() - end)\n",
        "      end = time.time()\n",
        "  \n",
        "      # Print model accuracy \n",
        "      if i % 25 == 0:\n",
        "          \n",
        "        print('Validate: [{0}/{1}]\\t'\n",
        "              'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "              'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(i, len(val_loader), \n",
        "                                                              batch_time=batch_time, loss=losses))\n",
        "  \n",
        "    print('Completed Validation Step')\n",
        "    return losses.avg\n",
        "\n",
        "\n",
        "\n",
        "# Define the train function \n",
        "def training(train_loader, model, criterion, optimizer, epoch):\n",
        "    \n",
        "  print('Training epoch {}'.format(epoch))\n",
        "  \n",
        "  # Set the model to train mode\n",
        "  model.train()\n",
        "  \n",
        "  # Prepare value counters and timers\n",
        "  batch_time, data_time, losses = AverageMeter(), AverageMeter(), AverageMeter()\n",
        "  end = time.time()\n",
        "  \n",
        "  for i, (input_image, target_image) in enumerate(train_loader):\n",
        "    \n",
        "    # If GPU available then use it\n",
        "    if use_gpu: \n",
        "        \n",
        "        input_image, target_image = input_image.cuda(), target_image.cuda()\n",
        "        model = model.cuda()\n",
        "        criterion = criterion.cuda()\n",
        "\n",
        "    # Record time to load data \n",
        "    data_time.update(time.time() - end)\n",
        "    \n",
        "    # Run validation pass on the model  \n",
        "    predicted = model(input_image)\n",
        "    \n",
        "    # Calculate the losses \n",
        "    loss = criterion(predicted, target_image)\n",
        "    losses.update(loss.item(), input_image.size(0))\n",
        "\n",
        "    # Compute gradient and optimize in backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Record time to do forward and backward passes\n",
        "    batch_time.update(time.time() - end)\n",
        "    end = time.time()\n",
        "\n",
        "    # Print model accuracy \n",
        "    if i % 25 == 0:\n",
        "      print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "            'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "            'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "            'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
        "              epoch, i, len(train_loader), batch_time=batch_time,\n",
        "             data_time=data_time, loss=losses)) \n",
        "\n",
        "  print('Completed training epoch {}'.format(epoch))\n",
        "\n",
        "  return losses.avg\n",
        "    "
      ],
      "metadata": {
        "id": "GCQHQ4sgbHg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start the training process \n",
        "\n",
        "1. Begin training the model using the hyperparameters defined above and see model performance on validation data"
      ],
      "metadata": {
        "id": "qXIgu51gj-27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# Start the process of training model\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    # Train model for every epoch and the call validation to track model performance \n",
        "    trainLoss = training(train_dataloader, model, criterion, optimizer, epoch)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      valLoss = validation(val_dataloader, model, criterion)\n",
        "      \n",
        "    # Save best model\n",
        "    if valLoss < best_loss:\n",
        "      best_loss = valLoss\n",
        "      torch.save(model.state_dict(), 'models/BasicModel-epoch-{}.pt'.format(epoch+1))\n",
        "\n",
        "    # Save the train and val loss \n",
        "    train_losses.append(trainLoss)\n",
        "    val_losses.append(valLoss)\n",
        "      \n",
        "      "
      ],
      "metadata": {
        "id": "dFJtPn6xbIFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test model performance and Visualise results"
      ],
      "metadata": {
        "id": "y_p7mqS06DUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the train loss and val loss curves \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from skimage import color, io\n",
        "import torchvision.utils\n",
        "import matplotlib.pyplot as plt\n",
        "plt.ion()\n",
        "\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Val loss')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b_J-o42x2vcr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "5009af8c-bde3-4530-f3be-2c65ab55b821"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-da1871741158>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Val loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_losses' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x360 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Model Path \n",
        "model_path = '/content/drive/MyDrive/Object Detection/Image Colorization/ResNetUNetModel-epoch-after-26-24.pt'\n",
        "\n",
        "# Load Model and set to evaluation mode\n",
        "model = ResNetUNet(3)\n",
        "\n",
        "model.load_state_dict(torch.load(model_path,map_location=torch.device('cpu')))\n",
        "model.eval()\n",
        "\n",
        "# Define the test path \n",
        "test_path = \"/content/drive/MyDrive/Object Detection/Image Colorization/landscape_images/landscape_images/test_eleazar\"\n",
        "\n",
        "# Train Dataloader\n",
        "test_data = ColorizeData(test_path)\n",
        "test_dataloader = DataLoader(test_data, batch_size=1, shuffle=True)\n",
        "\n",
        "for i, (input_image, target_image) in enumerate(test_dataloader):\n",
        "        \n",
        "    losses = AverageMeter()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        if use_gpu:\n",
        "          input_image = input_image.cuda()\n",
        "          target_image = target_image.cuda()\n",
        "          model = model.cuda()\n",
        "\n",
        "        predicted = model(input_image)\n",
        "\n",
        "        # Calculate the loss \n",
        "        loss = criterion(predicted, target_image)\n",
        "        losses.update(loss.item(), input_image.size(0))\n",
        "\n",
        "    # plot images\n",
        "    fig, ax = plt.subplots(figsize=(15, 15), nrows=1, ncols=2)\n",
        "  \n",
        "    predicted = predicted[0,:,:,:].cpu().numpy()\n",
        "    target_image = target_image[0,:,:,:].cpu().numpy()\n",
        "\n",
        "    ax[0].imshow(np.transpose(predicted, (1,2,0)))\n",
        "    ax[0].title.set_text('Predicted')\n",
        "    ax[1].imshow(np.transpose(target_image, (1,2,0)))\n",
        "    ax[1].title.set_text('Ground Truth')\n",
        "    plt.show()\n",
        "    \n",
        "print('Loss: %f' % (losses.avg))"
      ],
      "metadata": {
        "id": "gWBEbH3u-Pud",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "outputId": "c3ef7692-5860-431e-e607-a021ab074e22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-7dd0199a174a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNetUNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1498\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1499\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNetUNet:\n\tMissing key(s) in state_dict: \"conv0.0.weight\", \"conv0.1.weight\", \"conv0.1.bias\", \"conv0.1.running_mean\", \"conv0.1.running_var\". \n\tUnexpected key(s) in state_dict: \"batchnorm0.weight\", \"batchnorm0.bias\", \"batchnorm0.running_mean\", \"batchnorm0.running_var\", \"batchnorm0.num_batches_tracked\", \"conv0.weight\". \n\tsize mismatch for convOriginal0.0.weight: copying a param with shape torch.Size([64, 1, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 3, 3, 3])."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate psnr\n",
        "PSNR = 10*np.log(255**2/losses.avg) / np.log(10)\n",
        "print(PSNR)\n",
        "\n"
      ],
      "metadata": {
        "id": "0Q7YvtOUNOlp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fd5de71-3185-4744-c092-aecef6449bf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "58.137399765304274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchviz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLUo4vb5dibf",
        "outputId": "ca56eb63-f5b9-4f5b-e92e-196b8624da42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchviz) (1.11.0+cu113)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (4.2.0)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4150 sha256=1f9e15c253fda8324ab1c763f850b219f5af71bdb268d16729090813f85ce143\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/38/f5/dc4f85c3909051823df49901e72015d2d750bd26b086480ec2\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchviz \n",
        "from torchviz import *\n",
        "from torchviz import make_dot, make_dot_from_trace\n",
        "\n",
        "x = torch.randn(1, 1, 256, 256)\n",
        "\n",
        "model = BasicNet()\n",
        "\n",
        "make_dot(model(x), params=dict(model.named_parameters())).render(\"basic.png\",format=\"png\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nbiuqcziPpBN",
        "outputId": "adec1b31-6157-4e13-fd6a-71fab89cd691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'basic.png.png'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-ZH8YlX_dtno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(1, 1, 256, 256)\n",
        "\n",
        "model = ColorizationModel()\n",
        "\n",
        "make_dot(model(x), params=dict(model.named_parameters())).render(\"colorization.png\",format=\"png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Ot9ktqSLMQEQ",
        "outputId": "6c928c57-7e17-4a09-ca10-ea5d3b397db5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'colorization.png.png'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RvY6xmXBdyk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(1, 3, 256, 256)\n",
        "\n",
        "model = ResNetUNet(3)\n",
        "\n",
        "make_dot(model(x), params=dict(model.named_parameters())).render(\"resnet\",format=\"png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OzsZkMV1RyNW",
        "outputId": "c1a8e672-5fba-4da3-ee7c-82be076a9f0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'resnet.png'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Y5oLg1rXdzqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(1, 1, 256, 256)\n",
        "\n",
        "model = UNet()\n",
        "\n",
        "make_dot(model(x), params=dict(model.named_parameters())).render(\"unet.png\",format=\"png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "E_JZdPqfR72_",
        "outputId": "527fbbff-7791-4087-b62e-ce4ec8e10acd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'unet.png.png'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    }
  ]
}